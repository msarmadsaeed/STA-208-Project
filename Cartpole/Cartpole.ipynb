{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole game with different function approximators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required modules\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Ridge, SGDRegressor, BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two agent models. One uses linear function approximators and the other uses neural network approximator for Q learning based RL problem. For linear appriximators, we used whole replay memory for training the agent. However for neural netowork based agent we used randm batch update from the replay memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Q_Linear_approximator:\n",
    "    def __init__(self, state_size, action_size, model):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = model\n",
    "        self.isFit = False\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        if self.isFit == True:\n",
    "            act_values = self.model.predict(state)\n",
    "        else:\n",
    "            act_values = np.zeros(self.action_size).reshape(1, -1)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size): # agent trained with stored samples in this function \n",
    "        minibatch = random.sample(self.memory, int(len(self.memory)))\n",
    "        X=[]\n",
    "        targets=[]\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                if self.isFit:\n",
    "                    target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "                else:\n",
    "                    target=reward\n",
    "            if self.isFit:\n",
    "                target_f = self.model.predict(state)\n",
    "            else:\n",
    "                target_f = np.zeros(self.action_size).reshape(1, -1)\n",
    "            target_f[0][action] = target\n",
    "            \n",
    "            X.append(list(state[0]))\n",
    "            targets.append(target_f[0])\n",
    "        self.model.fit(X, targets)\n",
    "        self.isFit=True\n",
    "        # Epsilon decay method is user here in order to let agent to stick on learned actions\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network model agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_NN_approximator:\n",
    "    def __init__(self, state_size, action_size, model):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = model\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "    def replay(self, batch_size): # agent trained with stored samples in this function \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model(state_size, action_size):\n",
    "    # Neural Net for Deep-Q learning Model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse',optimizer=Adam(lr=0.001))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main learning function\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "#NN agent model is created and agent activated\n",
    "NN_learning=NN_model(state_size, action_size)\n",
    "agent = Q_NN_approximator(state_size, action_size, NN_learning)\n",
    "\n",
    "#choose one of the models below and activate the agent\n",
    "#model=MultiOutputRegressor(LGBMRegressor(n_estimators=100, n_jobs=-1))\n",
    "#model=MultiOutputRegressor(XGBRegressor())\n",
    "#model=MultiOutputRegressor(Ridge(alpha=0.1))\n",
    "#model=MultiOutputRegressor(KNeighborsRegressor(n_neighbors=10))\n",
    "#agent = Q_Linear_approximator(state_size, action_size, model)\n",
    "\n",
    "\n",
    "filename=\"CartpoleQ_approximate.txt\" #saeve the results to a text file\n",
    "done = False\n",
    "batch_size = 32\n",
    "EPISODES = 100\n",
    "\n",
    "for e in range(EPISODES):\n",
    "#    env.render()\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "#        reward = reward if not done else -10\n",
    "        reward = reward if not done else -reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, EPISODES, time, agent.epsilon))\n",
    "#            with open(filename, \"a\") as f:\n",
    "#                f.write(\"Simulation {}: Total score {}\\n\".format(e, time))\n",
    "            break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discussed our results on our final report instead of here. Please refer to STA_208_Project_Report.pdf file in the main directory of repo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
