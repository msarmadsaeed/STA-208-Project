{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import tensorflow\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, Flatten, Dense,TimeDistributed,LSTM\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "BATCH_SIZE = 32  # Mini batch size\n",
    "LOAD_NETWORK = False\n",
    "TRAIN = True\n",
    "SAVE_INTERVAL = 1000  # The frequency with which the network is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_WIDTH = 84  # Resized frame width\n",
    "FRAME_HEIGHT = 84  # Resized frame height\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, num_actions):\n",
    "        self.ENV_NAME = 'Breakout-v0'  # Game and its version\n",
    "        self.SAVE_NETWORK_PATH = 'saved_networks/' + self.ENV_NAME\n",
    "        self.SAVE_SUMMARY_PATH = 'summary/' + self.ENV_NAME\n",
    "        self.EXPLORATION_STEPS = 1000000  # Steps for linearly decreasing epsilon\n",
    "        self.GAMMA = 0.99  # Discount factor\n",
    "        self.INITIAL_EPSILON = 1.0  # Initial value of epsilon in epsilon-greedy\n",
    "        self.FINAL_EPSILON = 0.1  # Final value of epsilon in epsilon-greedy\n",
    "        self.TRAIN_INTERVAL = 4  # The agent selects 4 actions between successive updates\n",
    "        \n",
    "        # Parameters used for summary\n",
    "        self.total_reward = 0\n",
    "        self.total_q_max = 0\n",
    "        self.total_loss = 0\n",
    "        self.duration = 0\n",
    "        self.episode = 0\n",
    "        \n",
    "        #setting the decay\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = self.INITIAL_EPSILON\n",
    "        self.epsilon_step = (self.INITIAL_EPSILON - self.FINAL_EPSILON) / self.EXPLORATION_STEPS\n",
    "        self.t = 0\n",
    "\n",
    "        # Create replay memory\n",
    "        self.replay_memory = deque()\n",
    "\n",
    "        # Create q network\n",
    "        self.s, self.q_values, q_network = self.build_model()\n",
    "        q_network_weights = q_network.trainable_weights\n",
    "        \n",
    "        # Create target network\n",
    "        self.st, self.target_q_values, target_network = self.build_model()\n",
    "        target_network_weights = target_network.trainable_weights\n",
    "\n",
    "        # Define target network update operation\n",
    "        self.update_target_network = [target_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_network_weights))]\n",
    "\n",
    "        # Define loss and gradient update operation\n",
    "        self.a, self.y, self.loss, self.grads_update = self.build_training_op(q_network_weights)\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.saver = tf.train.Saver(q_network_weights)\n",
    "\n",
    "        if not os.path.exists(self.SAVE_NETWORK_PATH):\n",
    "            os.makedirs(self.SAVE_NETWORK_PATH)\n",
    "\n",
    "        self.sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        # Load network\n",
    "        if LOAD_NETWORK:\n",
    "            self.load()\n",
    "\n",
    "        # Initialize target network\n",
    "        self.sess.run(self.update_target_network)\n",
    "        \n",
    "   \n",
    "    STATE_LENGTH = 4  # Number of most recent frames to produce the input to the network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(TimeDistributed(Convolution2D(32, 8, 8, subsample=(4, 4), activation='relu', input_shape=(STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT))))\n",
    "        model.add(TimeDistributed(Convolution2D(64, 4, 4, subsample=(2, 2), activation='relu')))\n",
    "        model.add(TimeDistributed(Convolution2D(64, 3, 3, subsample=(1, 1), activation='relu')))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "        model.add(LSTM(512))\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.num_actions))\n",
    "\n",
    "        s = tf.placeholder(tf.float32, [None,None, STATE_LENGTH, FRAME_WIDTH, FRAME_HEIGHT])\n",
    "        q_values = model(s)\n",
    "\n",
    "        return s, q_values, model\n",
    "\n",
    "    def build_training_op(self, q_network_weights):\n",
    "        a = tf.placeholder(tf.int64, [None])\n",
    "        y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        # Convert action to one hot vector\n",
    "        a_one_hot = tf.one_hot(a, self.num_actions, 1.0, 0.0)\n",
    "        q_value = tf.reduce_sum(tf.multiply(self.q_values, a_one_hot), reduction_indices=1)\n",
    "\n",
    "        # Clip the error, the loss is quadratic when the error is in (-1, 1), and linear outside of that region\n",
    "        error = tf.abs(y - q_value)\n",
    "        quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "        \n",
    "        MOMENTUM = 0.95  \n",
    "        LEARNING_RATE = 0.00025  # Learning rate used by RMSProp\n",
    "        MIN_GRAD = 0.01  # Constant added to the squared gradient in the denominator of the RMSProp update\n",
    "        optimizer = tf.train.RMSPropOptimizer(LEARNING_RATE, momentum=MOMENTUM, epsilon=MIN_GRAD)\n",
    "        grads_update = optimizer.minimize(loss, var_list=q_network_weights)\n",
    "\n",
    "        return a, y, loss, grads_update\n",
    "\n",
    "  \n",
    "    INITIAL_REPLAY_SIZE = 20000  # Number of steps to populate the replay memory before training starts\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        if self.epsilon >= random.random() or self.t < self.INITIAL_REPLAY_SIZE:\n",
    "            action = random.randrange(self.num_actions)\n",
    "        else:\n",
    "            state = np.reshape([state], (1, 4, 84, 84))\n",
    "            action = np.argmax(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "\n",
    "        # Decreasing espsilon over time\n",
    "        if self.epsilon > self.FINAL_EPSILON and self.t >= self.INITIAL_REPLAY_SIZE:\n",
    "            self.epsilon -= self.epsilon_step\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def remember(self,state, action, reward, next_state, terminal):\n",
    "        self.replay_memory.append((state, action, reward, next_state, terminal))\n",
    "    \n",
    "    def act(self, state, action, reward, terminal, observation):\n",
    "        next_state = np.append(state[1:, :, :], observation, axis=0)\n",
    "\n",
    "        # Clip all positive rewards at 1 and all negative rewards at -1, leaving 0 rewards unchanged\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "\n",
    "        # Store transition in replay memory\n",
    "        NUM_REPLAY_MEMORY = 400000  # Number of replay memory the agent uses for training\n",
    "        \n",
    "        self.remember(state, action, reward, next_state, terminal)\n",
    "        \n",
    "        if len(self.replay_memory) > NUM_REPLAY_MEMORY:\n",
    "            self.replay_memory.popleft()\n",
    "\n",
    "        TARGET_UPDATE_INTERVAL = 10000  # The frequency with which the target network is updated\n",
    "        if self.t >= self.INITIAL_REPLAY_SIZE:\n",
    "            # Train network\n",
    "            if self.t % self.TRAIN_INTERVAL == 0:\n",
    "                self.replay()\n",
    "\n",
    "            # Update target network\n",
    "            if self.t % TARGET_UPDATE_INTERVAL == 0:\n",
    "                self.sess.run(self.update_target_network)\n",
    "\n",
    "            # Save network\n",
    "            if self.t % SAVE_INTERVAL == 0:\n",
    "                self.save()\n",
    "\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        state = np.reshape([state],( 1,4, 84, 84 ))\n",
    "        self.total_q_max += np.max(self.q_values.eval(feed_dict={self.s: [np.float32(state / 255.0)]}))\n",
    "        self.duration += 1\n",
    "\n",
    "        if terminal:\n",
    "            # Debug\n",
    "            if self.t < self.INITIAL_REPLAY_SIZE:\n",
    "                mode = 'random'\n",
    "            elif self.INITIAL_REPLAY_SIZE <= self.t < self.INITIAL_REPLAY_SIZE + self.EXPLORATION_STEPS:\n",
    "                mode = 'explore'\n",
    "            else:\n",
    "                mode = 'exploit'\n",
    "            print('EPISODE: {0:6d} / TIMESTEP: {1:8d} / DURATION: {2:5d} / EPSILON: {3:.5f} / TOTAL_REWARD: {4:3.0f} / AVG_MAX_Q: {5:2.4f} / AVG_LOSS: {6:.5f} / MODE: {7}'.format(\n",
    "                self.episode + 1, self.t, self.duration, self.epsilon,\n",
    "                self.total_reward, self.total_q_max / float(self.duration),\n",
    "                self.total_loss / (float(self.duration) / float(self.TRAIN_INTERVAL)), mode))\n",
    "            with open(\"BreakoutgameDQN.txt\", \"a\") as f:\n",
    "                f.write(\"Simulation {}: Total reward {}  total loss {}\\n\".format(str(self.episode + 1), self.total_reward, self.total_reward))\n",
    "\n",
    "            self.total_reward = 0\n",
    "            self.total_q_max = 0\n",
    "            self.total_loss = 0\n",
    "            self.duration = 0\n",
    "            self.episode += 1\n",
    "\n",
    "        self.t += 1\n",
    "\n",
    "        return next_state\n",
    "    \n",
    "    #funtion for training network\n",
    "    def replay(self):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        terminal_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Sample random minibatch of transition from replay memory\n",
    "        minibatch = random.sample(self.replay_memory, BATCH_SIZE)\n",
    "        for data in minibatch:\n",
    "            xz = np.reshape([data[0]], (1, 4, 84, 84))\n",
    "            state_batch.append(xz)\n",
    "            action_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            xy = np.reshape([data[3]], (1, 4, 84, 84))\n",
    "            next_state_batch.append(xy)\n",
    "            terminal_batch.append(data[4])\n",
    "\n",
    "        # Convert True to 1, False to 0\n",
    "        terminal_batch = np.array(terminal_batch) + 0\n",
    "\n",
    "        target_q_values_batch = self.target_q_values.eval(feed_dict={self.st: np.float32(np.array(next_state_batch) / 255.0)})\n",
    "        y_batch = reward_batch + (1 - terminal_batch) * self.GAMMA * np.max(target_q_values_batch, axis=1)\n",
    "\n",
    "        loss, _ = self.sess.run([self.loss, self.grads_update], feed_dict={\n",
    "            self.s: np.float32(np.array(state_batch) / 255.0),\n",
    "            self.a: action_batch,\n",
    "            self.y: y_batch\n",
    "        })\n",
    "\n",
    "        self.total_loss += loss\n",
    "        \n",
    "    def load(self):\n",
    "        checkpoint = tf.train.get_checkpoint_state(self.SAVE_NETWORK_PATH)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, checkpoint.model_checkpoint_path)\n",
    "            \n",
    "    def save(self):\n",
    "        save_path = self.saver.save(self.sess, self.SAVE_NETWORK_PATH + '/' + self.ENV_NAME, global_step=self.t)\n",
    "        print('Successfully saved: ' + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarmad Saeed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (8, 8), activation=\"relu\", input_shape=(4, 84, 84..., strides=(4, 4))`\n",
      "C:\\Users\\Sarmad Saeed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (4, 4), activation=\"relu\", strides=(2, 2))`\n",
      "C:\\Users\\Sarmad Saeed\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", strides=(1, 1))`\n",
      "C:\\Users\\Sarmad Saeed\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:      1 / TIMESTEP:      183 / DURATION:   184 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      2 / TIMESTEP:      352 / DURATION:   169 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      3 / TIMESTEP:      633 / DURATION:   281 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0018 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      4 / TIMESTEP:      811 / DURATION:   178 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      5 / TIMESTEP:      988 / DURATION:   177 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      6 / TIMESTEP:     1226 / DURATION:   238 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      7 / TIMESTEP:     1423 / DURATION:   197 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      8 / TIMESTEP:     1764 / DURATION:   341 / EPSILON: 1.00000 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0017 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:      9 / TIMESTEP:     2015 / DURATION:   251 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     10 / TIMESTEP:     2244 / DURATION:   229 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0022 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     11 / TIMESTEP:     2640 / DURATION:   396 / EPSILON: 1.00000 / TOTAL_REWARD:   4 / AVG_MAX_Q: 0.0018 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     12 / TIMESTEP:     3064 / DURATION:   424 / EPSILON: 1.00000 / TOTAL_REWARD:   4 / AVG_MAX_Q: 0.0020 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     13 / TIMESTEP:     3483 / DURATION:   419 / EPSILON: 1.00000 / TOTAL_REWARD:   4 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     14 / TIMESTEP:     3808 / DURATION:   325 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0022 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     15 / TIMESTEP:     4065 / DURATION:   257 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0023 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     16 / TIMESTEP:     4236 / DURATION:   171 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     17 / TIMESTEP:     4454 / DURATION:   218 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     18 / TIMESTEP:     4661 / DURATION:   207 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0022 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     19 / TIMESTEP:     4834 / DURATION:   173 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     20 / TIMESTEP:     5176 / DURATION:   342 / EPSILON: 1.00000 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0025 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     21 / TIMESTEP:     5392 / DURATION:   216 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0028 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     22 / TIMESTEP:     5610 / DURATION:   218 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0025 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     23 / TIMESTEP:     5781 / DURATION:   171 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     24 / TIMESTEP:     5964 / DURATION:   183 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0027 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     25 / TIMESTEP:     6143 / DURATION:   179 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0025 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     26 / TIMESTEP:     6488 / DURATION:   345 / EPSILON: 1.00000 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0021 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     27 / TIMESTEP:     6654 / DURATION:   166 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0025 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     28 / TIMESTEP:     6905 / DURATION:   251 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0019 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     29 / TIMESTEP:     7138 / DURATION:   233 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0019 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     30 / TIMESTEP:     7417 / DURATION:   279 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0021 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     31 / TIMESTEP:     7631 / DURATION:   214 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     32 / TIMESTEP:     7999 / DURATION:   368 / EPSILON: 1.00000 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     33 / TIMESTEP:     8239 / DURATION:   240 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0020 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     34 / TIMESTEP:     8479 / DURATION:   240 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0027 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     35 / TIMESTEP:     8692 / DURATION:   213 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     36 / TIMESTEP:     8967 / DURATION:   275 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0019 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     37 / TIMESTEP:     9248 / DURATION:   281 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     38 / TIMESTEP:     9514 / DURATION:   266 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0022 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     39 / TIMESTEP:     9831 / DURATION:   317 / EPSILON: 1.00000 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0019 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     40 / TIMESTEP:    10113 / DURATION:   282 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0021 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     41 / TIMESTEP:    10452 / DURATION:   339 / EPSILON: 1.00000 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0019 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     42 / TIMESTEP:    10630 / DURATION:   178 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0025 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     43 / TIMESTEP:    10797 / DURATION:   167 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     44 / TIMESTEP:    10965 / DURATION:   168 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     45 / TIMESTEP:    11278 / DURATION:   313 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0020 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     46 / TIMESTEP:    11489 / DURATION:   211 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     47 / TIMESTEP:    11657 / DURATION:   168 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0022 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     48 / TIMESTEP:    12013 / DURATION:   356 / EPSILON: 1.00000 / TOTAL_REWARD:   4 / AVG_MAX_Q: 0.0022 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     49 / TIMESTEP:    12247 / DURATION:   234 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     50 / TIMESTEP:    12419 / DURATION:   172 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0025 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     51 / TIMESTEP:    12742 / DURATION:   323 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0020 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     52 / TIMESTEP:    12978 / DURATION:   236 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0021 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     53 / TIMESTEP:    13165 / DURATION:   187 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     54 / TIMESTEP:    13415 / DURATION:   250 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0021 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     55 / TIMESTEP:    13591 / DURATION:   176 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE:     56 / TIMESTEP:    13872 / DURATION:   281 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0027 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     57 / TIMESTEP:    14177 / DURATION:   305 / EPSILON: 1.00000 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0020 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     58 / TIMESTEP:    14415 / DURATION:   238 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0022 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     59 / TIMESTEP:    14623 / DURATION:   208 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0025 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     60 / TIMESTEP:    14860 / DURATION:   237 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0023 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     61 / TIMESTEP:    15156 / DURATION:   296 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0021 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     62 / TIMESTEP:    15536 / DURATION:   380 / EPSILON: 1.00000 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0027 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     63 / TIMESTEP:    15772 / DURATION:   236 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0021 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     64 / TIMESTEP:    16015 / DURATION:   243 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     65 / TIMESTEP:    16189 / DURATION:   174 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     66 / TIMESTEP:    16398 / DURATION:   209 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     67 / TIMESTEP:    16609 / DURATION:   211 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0025 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     68 / TIMESTEP:    16891 / DURATION:   282 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0017 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     69 / TIMESTEP:    17258 / DURATION:   367 / EPSILON: 1.00000 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0022 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     70 / TIMESTEP:    17450 / DURATION:   192 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0025 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     71 / TIMESTEP:    17761 / DURATION:   311 / EPSILON: 1.00000 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0022 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     72 / TIMESTEP:    17969 / DURATION:   208 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     73 / TIMESTEP:    18199 / DURATION:   230 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     74 / TIMESTEP:    18441 / DURATION:   242 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0026 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     75 / TIMESTEP:    18722 / DURATION:   281 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0022 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     76 / TIMESTEP:    18920 / DURATION:   198 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0024 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     77 / TIMESTEP:    19094 / DURATION:   174 / EPSILON: 1.00000 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0023 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     78 / TIMESTEP:    19329 / DURATION:   235 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0020 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     79 / TIMESTEP:    19562 / DURATION:   233 / EPSILON: 1.00000 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0021 / AVG_LOSS: 0.00000 / MODE: random\n",
      "EPISODE:     80 / TIMESTEP:    19829 / DURATION:   267 / EPSILON: 1.00000 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0018 / AVG_LOSS: 0.00000 / MODE: random\n",
      "Successfully saved: saved_networks/Breakout-v0/Breakout-v0-20000\n",
      "EPISODE:     81 / TIMESTEP:    20193 / DURATION:   364 / EPSILON: 0.99983 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0033 / AVG_LOSS: 0.00174 / MODE: explore\n",
      "EPISODE:     82 / TIMESTEP:    20431 / DURATION:   238 / EPSILON: 0.99961 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0095 / AVG_LOSS: 0.00341 / MODE: explore\n",
      "EPISODE:     83 / TIMESTEP:    20670 / DURATION:   239 / EPSILON: 0.99940 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0156 / AVG_LOSS: 0.00209 / MODE: explore\n",
      "EPISODE:     84 / TIMESTEP:    20848 / DURATION:   178 / EPSILON: 0.99924 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0115 / AVG_LOSS: 0.00245 / MODE: explore\n",
      "Successfully saved: saved_networks/Breakout-v0/Breakout-v0-21000\n",
      "EPISODE:     85 / TIMESTEP:    21041 / DURATION:   193 / EPSILON: 0.99906 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0118 / AVG_LOSS: 0.00451 / MODE: explore\n",
      "EPISODE:     86 / TIMESTEP:    21319 / DURATION:   278 / EPSILON: 0.99881 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0159 / AVG_LOSS: 0.00402 / MODE: explore\n",
      "EPISODE:     87 / TIMESTEP:    21629 / DURATION:   310 / EPSILON: 0.99853 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0110 / AVG_LOSS: 0.00141 / MODE: explore\n",
      "EPISODE:     88 / TIMESTEP:    21868 / DURATION:   239 / EPSILON: 0.99832 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0095 / AVG_LOSS: 0.00313 / MODE: explore\n",
      "Successfully saved: saved_networks/Breakout-v0/Breakout-v0-22000\n",
      "EPISODE:     89 / TIMESTEP:    22116 / DURATION:   248 / EPSILON: 0.99809 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0152 / AVG_LOSS: 0.00401 / MODE: explore\n",
      "EPISODE:     90 / TIMESTEP:    22350 / DURATION:   234 / EPSILON: 0.99788 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0087 / AVG_LOSS: 0.00055 / MODE: explore\n",
      "EPISODE:     91 / TIMESTEP:    22662 / DURATION:   312 / EPSILON: 0.99760 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0073 / AVG_LOSS: 0.00180 / MODE: explore\n",
      "EPISODE:     92 / TIMESTEP:    22966 / DURATION:   304 / EPSILON: 0.99733 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0102 / AVG_LOSS: 0.00164 / MODE: explore\n",
      "Successfully saved: saved_networks/Breakout-v0/Breakout-v0-23000\n",
      "EPISODE:     93 / TIMESTEP:    23312 / DURATION:   346 / EPSILON: 0.99702 / TOTAL_REWARD:   3 / AVG_MAX_Q: 0.0143 / AVG_LOSS: 0.00288 / MODE: explore\n",
      "EPISODE:     94 / TIMESTEP:    23480 / DURATION:   168 / EPSILON: 0.99687 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0104 / AVG_LOSS: 0.00222 / MODE: explore\n",
      "EPISODE:     95 / TIMESTEP:    23658 / DURATION:   178 / EPSILON: 0.99671 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0118 / AVG_LOSS: 0.00245 / MODE: explore\n",
      "EPISODE:     96 / TIMESTEP:    23843 / DURATION:   185 / EPSILON: 0.99654 / TOTAL_REWARD:   0 / AVG_MAX_Q: 0.0078 / AVG_LOSS: 0.00235 / MODE: explore\n",
      "Successfully saved: saved_networks/Breakout-v0/Breakout-v0-24000\n",
      "EPISODE:     97 / TIMESTEP:    24050 / DURATION:   207 / EPSILON: 0.99635 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0106 / AVG_LOSS: 0.00301 / MODE: explore\n",
      "EPISODE:     98 / TIMESTEP:    24275 / DURATION:   225 / EPSILON: 0.99615 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0134 / AVG_LOSS: 0.00386 / MODE: explore\n",
      "EPISODE:     99 / TIMESTEP:    24549 / DURATION:   274 / EPSILON: 0.99591 / TOTAL_REWARD:   2 / AVG_MAX_Q: 0.0129 / AVG_LOSS: 0.00274 / MODE: explore\n",
      "EPISODE:    100 / TIMESTEP:    24787 / DURATION:   238 / EPSILON: 0.99569 / TOTAL_REWARD:   1 / AVG_MAX_Q: 0.0084 / AVG_LOSS: 0.00210 / MODE: explore\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-618306c59bef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mprocessed_observation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_observation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFRAME_WIDTH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFRAME_HEIGHT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                 \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessed_observation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-a2fba1c18f31>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, action, reward, terminal, observation)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m84\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m84\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_q_max\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m     \"\"\"\n\u001b[1;32m--> 798\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mexperimental_ref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5405\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5406\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 5407\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('Breakout-v0')\n",
    "    agent = Agent(num_actions=env.action_space.n)\n",
    "    \n",
    "    EPISODES = 30000  # times the game is played\n",
    "    NO_OP_STEPS = 30  # Maximum number of \"do nothing\" actions to be performed by the agent at the start of an episode\n",
    "    STATE_LENGTH= 4\n",
    "    \n",
    "    if TRAIN:  # Train mode\n",
    "        for _ in range(EPISODES):\n",
    "            terminal = False\n",
    "            observation = env.reset()\n",
    "            for _ in range(random.randint(1, NO_OP_STEPS)):\n",
    "                last_observation = observation\n",
    "                observation, _, _, _ = env.step(0)  # Do nothing\n",
    "            processed_observation = np.maximum(observation, last_observation)\n",
    "            processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "            state = [processed_observation for _ in range(STATE_LENGTH)]\n",
    "            state =  np.stack(state, axis=0)\n",
    "            #state = agent.get_initial_state(observation, last_observation)\n",
    "            while not terminal:\n",
    "                last_observation = observation\n",
    "                action = agent.get_action(state)\n",
    "                observation, reward, terminal, _ = env.step(action)\n",
    "                \n",
    "                # doing pre processing here                \n",
    "                processed_observation = np.maximum(observation, last_observation)\n",
    "                processed_observation = np.uint8(resize(rgb2gray(processed_observation), (FRAME_WIDTH, FRAME_HEIGHT)) * 255)\n",
    "                processed_observation = np.reshape(processed_observation, (1, FRAME_WIDTH, FRAME_HEIGHT))\n",
    "                \n",
    "                state = agent.act(state, action, reward, terminal, processed_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
